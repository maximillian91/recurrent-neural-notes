\section{Experiments}
\label{sec:experiments}

% \begin{description}
% 	\item [RNN] 10, 25, 50, 75, and 100 gated recurrent units.
% 	\item [Training] 200 epochs.
% 	\item [Learning curves] cost functions, accuracies, and some metric for weights.
% 	\item[Activations] of GRUs for tunes.
% 	\item[Evaluation] on test set as a table.
% 	\item [Reconstructions] of test examples.
% \end{description}
\subsection{Model setup}

The experiments were run for the normal RNN model using only original input and the extended RNN model using the previous prediction step output as input. For regularization, 20\% dropout was applied right before the GRU layer in the input network and right af the GRU layer in output network respectively in both model architectures. 50\% dropout was also applied right after the GRU layer in the normal RNN, but not in the extended model, as the recursive call of output network cannot work with nondeterministic outputs. 

The two models with and without dropout are tested for 10, 25, 50, 75, and 100 gated recurrent units, so scaling of performance can be investigated. 

All are trained for 200 epochs, as a clear convergence in the performance measures were seen after 150 epochs of training. \\ \\
\subsection{Performance measures}

The performance measures are the prediction accuracy and the two categorical cross entropy measures over pitch and duration classes are recorded for each epoch and seen in the learning curves.

Other measures used for model investigation are the mean, frobenius norm and positive value fractions over both the horizontal weights (hidden to hidden state) and vertical weights (input to hidden state), where the trend in the overall magnitude and sign of the weights are expected to be seen. \\ \\
\subsection{Evaluation and reconstructions}

After training, the models were evaluated on 2 test songs, handpicked out of the unseen test set. The notes in the original melody (input) are highlighted by colors corresponding to the GRU activations, so patterns, like scale and rhythmical motifs, leading to higher activation can be localized and the functionality of the GRU identified.

To investigate the distribution of the pitch and duration classes, the frequencies of each class in the original data and the reconstructions of the full data set from each model are represented in a histogram in Figure~\ref{fig:learning_curves}

All reconstructions can be translated back to the \texttt{music21} format, written to a midi file, played, written as a musical score and compared against the original melody.

\begin{table*}
    \centering
    \caption{
        The test evalutation measures.
    }
    \label{tab:test_eval}
    % \setlength{\tabcolsep}{1em}
    \sisetup{
        table-number-alignment = center
    }
    \begin{tabular}{
            l@{}
            S[table-format = 1.0]
            S[table-format = 0.2]
            S[table-format = 1.0]
            S[table-format = 1.3]
            S[table-format = 1.3]
            S[table-format = 2.2]
            S[table-format = 2.2]
        }
        \toprule
        & {Model} 
        & {$p\idx{dropout}$}
        & {$L_2$}
        & {$L\idx{pitch}$}
        & {$L\idx{duration}$}
        & {$P\idx{pitch}$(\%)}
        & {$P\idx{duration}$(\%)} \\
        \midrule
        \input{../data/eval_table}
        \bottomrule
    \end{tabular}
\end{table*}

\begin{figure*}
    \centering
	\hspace*{\fill}
    \subbottom[\label{fig:learning_curves:cost}]{
        \includegraphics[width = .45\linewidth]{GRU_using_deterministic_previous_output_with_50p_dropout_gru_100_bs_10_e_200_cost}
    }
    \hfill
    \subbottom[\label{fig:learning_curves:acc}]{
        \includegraphics[width = .45\linewidth]{GRU_using_deterministic_previous_output_with_50p_dropout_gru_100_bs_10_e_200_acc}
    }
	\hspace*{\fill}
    \caption{Learning curves over categorical cross-entropies \subcaptionref{fig:learning_curves:cost} and accuracies \subcaptionref{fig:learning_curves:acc} for both training and validation sets (Model 2 with $N\idx{gru} = 100$ and dropout of $50\%$)
    }
    \label{fig:learning_curves}
\end{figure*}

\begin{figure*}
    \centering
    \setkeys{Gin}{height=.25\textheight}
	\hspace*{\fill}
    \subbottom[\label{fig:histogram:pitch}]{
        \includegraphics{pitch_freq_barplot}
    }
    \hfill
    \subbottom[\label{fig:histogram:duration}]{
        \includegraphics{duration_freq_barplot}
    }
	\hspace*{\fill}
    \caption{Histograms showing statistical frequency of pitch \subcaptionref{fig:histogram:pitch} and duration \subcaptionref{fig:histogram:duration} classes.)
    }
    \label{fig:learning_curves}
\end{figure*}

% \subsection*{EHR synopsis}

% We expect our model to be able to predict future diagnoses better than baseline methods like logistic regression and the frequency baseline test given in Doctor AI. At the same time, the model will learn to recognise specific patterns in the patient history that will enhance the models predictive power. These patterns can be investigated and used as important markers for doctors to analyse the patient history and whether later complications can be expected and avoided.

% In the worst-case scenario, we are not successful in achieving the above.

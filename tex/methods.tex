\section{Methods}
\label{sec:method}

\subsection{Conditional Distribution} % (fold)
\label{sub:conditional_distribution}
The next-step prediction of the output network will be based on the conditional distribution:
	\begin{gather}
		\begin{split}
		\MoveEqLeft[3]
				p\left(\vec{x}\order{t+1}\idx{p}|\vec{x}\order{t'\le t}\right) \\ 
				& = p\left(\vec{x}\order{0}\idx{p}\right) \prod^{t+1}_{t'=1} p\left(\vec{x}\order{t'}\idx{p}|\vec{x}\order{t'-1}\right) \label{}
		\end{split} 	
	\end{gather}
	where $\vec{x}\order{t}=\{\vec{x}\order{t}\idx{p}, \vec{x}\order{t}\idx{p}\}$ is the combined class feature vector of the two class feature vectors $\vec{x}\order{t}\idx{p}$ and $\vec{x}\order{t}\idx{d}$ at step $t$. The conditional distribution is similar for the duration classes, so the duration condition on both the previous pitches and durations encountered and likewise for the pitch. 

	The archictecture of the full network model is based on the assumption of these two conditional distributions.
% subsection conditional_distribution (end)

\subsection{Input network} % (fold)
\label{sub:input_network}
The input network merges the two class feature vectors, $\{\vec{x}\order{t}\idx{p}$ and $\vec{x}\order{t}\idx{p}\}$ into a two-hot-encoded vector, $\vec{x}\order{t}\idx{d}$, so the predictions of the output network can condition on both. The input network also provides the binary mask for each melody for the GRU-layer, so exceptions can be made when reaching the end of each melody.
% subsection input_network (end)

\subsection{Recurrent neural network} % (fold)
\label{sub:recurrent_neural_network}
	The main part of the models investigated is the recurrent neural network in the center transferring temporal information from the input network to the output network, so the next-step predictions from the output network can condition on both class types from all of the previous melody. 
	To enhance the long term memory, gated recurrent units (GRU) are used. These are receiving the two-hot-encoded input for each step, $t$, in the melody. Each GRU run through the melody step by step, calculating the hidden activations recursively by the activation functions:
	\begin{alignat}{2}
	 	& \text{Reset:} \; & r\order{t} &= \sigma_r(\vec{x}\order{t} W_{xr} + \vec{h}\order{t-1} W_{hr} + b_r) \label{gru:reset} \\
        & \text{Update:} \; & u\order{t} &= \sigma_u(\vec{x}\order{t} W_{xu} + \vec{h}\order{t-1} W_{hu} + b_u) \label{gru:update} \\
        & \text{Candidate:} \; & c\order{t} &= \sigma_c(\vec{x}\order{t} W_{xc} + r\order{t} \odot (\vec{h}\order{t-1} W_{hc}) + b_c) \label{gru:candidate} \\
        & \text{Activation:} \; & \vec{h}\order{t} &= (1 - u\order{t}) \odot \vec{h}\order{t-1} + u\order{t} \odot c\order{t} \label{gru:activation}
	\end{alignat}
	where $W_x$ are the vertical weights transforming the current input $\vec{x}\order{t}$ linearly and the $W_h$ are the horizontal weights transforming the previous activation linearly, so information about previous states can propagate horizontally through each step. The non-linear sigmoid functions are the logistic function ($\sigma(\vec{x})=(1+\eup^{-\vec{x}})^{-1}$) for the reset gate in equation~\eqref{gru:reset} and update gate in equation~\eqref{gru:update} and the hyperbolic tangent function for the candidate activation in equation~\eqref{gru:candidate}. The reset gate controls how much of the signal from the previous activation, $\vec{h}\order{t-1}$, will be a part of the current candidate activation, $c\order{t}$, in equation~\eqref{gru:candidate} and the update gate then controls the proportional mixing of $\vec{h}\order{t-1}$ and $c\order{t}$ in the new activation, $\vec{h}\order{t}$, in equation~\eqref{gru:activation}. This means that $(r\order{t}, u\order{t}) = (0,1)$ will cut off the signal from the previous states and $(r\order{t}, u\order{t}) = (1,0)$ will copy the previous activation entirely. This makes it possible for the model to condition the next prediction over long terms, like the overall tonality and common motifs, and short terms, like previous pitch interval jumps and current harmony (chords). The models ability to perform this temporal conditioning in a musical manner is what will be investigated later on. 


	When running the backprogation algorithm for adjusting the model parameters in recurrent neural networks, vanishing gradients can be a huge problem leading to premature convergence (stopped learning). This is due to neurones in the network being saturated, when the gradient of the activation function is close to zero near convergence along the tails. The gradients will therefore vanish after multiple multiplication in the backpropagation, where each gradient depends on the previous. 

	Exploding gradients can also occur when training recurrent neural networks, so a gradient clipping of 3 is performed.  
% subsection recurrent_neural_network (end)

\subsection{Output network} % (fold)
\label{sub:output_network}
	The output network is 2 standard feedforward neural networks with softmax activation functions for class probabilities. A neurones response to the hidden GRU states at time $t$ represents the probability of the next note at time $t+1$ belonging to a specific pitch class for the pitch network and duration class for the duration network.
	The output function for class $k$ at time $t+1$ is:
	\begin{equation}
		f(\vec{x})\order{t+1}_k = s\left(\mat{W}_k \vec{h}\order{t} + \vec{b}\right)%\frac{\eup^{\vec{h}\order{t}}}{\sum^K_{k=1}}
	\end{equation}
	where $s(\vec{x})$ is the softmax activation function, $\vec{h}\order{t}$ are the hidden GRU activations at time step $t$. As $\vec{h}_t$ is a non-linear function of all previous notes $\vec{x}\order{t'<t}$ indirectly through the previous activation $\vec{h}\order{t-1}$ and of the current note $\vec{x}_t$ directly through the input network,    
	
	\subsection{Extended model} % (fold)
	\label{sub:extended_model}
		By plugging in the next-step prediction of the output network to the next step input of the GRU layer an extra loop is formed which transfers additional information about the previous step and penalizes the model more for prediction errors.  
		Adding the most probable prediction $\tilde{\vec{x}}\order{t}$ from previous step to the original input, $\vec{x}\order{t}$ after weighting and transforming both through a ReLU dense layer will combine the new input $\hat{\vec{x}}\order{t}$, which will go into the activation functions in Equation~\eqref{gru:reset}-\eqref{gru:activation} in the exact same manner.
	% subsection extended_model (end)

	\subsection{Cost function} 
	The categorical cross entropy cost function for example $n$ is:
	\begin{equation}
		%L(\vec{x}\order{t}; \vec{f}) = \frac{1}{T}\sum^{T-1}_{t} \sum^{M}_{m} \left(\vec{x}_{m}\order{t+1} \log f(\vec{x}\order{t}) + \log(1-f(\vec{x}\order{t})  \right)
		L_n(\vec{p}, \vec{q}) = -\sum^{M}_{m} p_{n,m} \log (q_{n,m})
	\end{equation}
	summed over all $M$ classes, where $\vec{p}$ is the one-hot-encoded targets $\vec{x}\order{t+1}$ at timesteps $t\in\{1, T\}$. $\vec{q}=f(\vec{x}_n)$ is the softmax output from the network taking input $\vec{x}\order{t+1}$ at timesteps $t\in\{0, T-1\}$. All timesteps are treated as seperate datapoints, so the targets and outputs are unrolled to a 2D $([N \times [T-1] \text{ by } M)$-array and the loss for each are averaged over, $n$, the first dimension, after masking out the steps outside the melodies.

% subsection output_network (end)


\subsection*{EHR synopsis}

To apply the Deepr model to the EHR data, we will be working with text preprocessing methods, like one-hot encoding and word-embedding, on irregular time-series of diagnosis codes converted into sentences with time intervals between hospital visits inserted as special words (e.g., “1--3m” for 1 to 3 months). Therefore this task is comparable to applying convolutional neural networks (CNNs) for natural language processing problems, looking for local patterns and summing these up to a global classification.

The Doctor AI paper’s model is implemented using multi-hot encoding of all diagnoses and medications recorded at each hospital visit with a timestamp. A two-layer recurrent neural network (RNN) are used for predicting diagnoses given at and durations between future hospital visits. The predictive power of this model lies in its ability to connect diagnoses in long term patterns important to future diagnoses.

We will investigate the possibilities of using dilated causal convolutions as a cross between the CNN and RNN methods for predicting future diagnoses. Google DeepMind use these convolutions for predicting future audio samples in speech synthesis with WaveNet.

A. van den Oord et al.: “WaveNet: A Generative Model for Raw Audio”, September 2016.

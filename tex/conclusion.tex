\section{conclusion}
\label{sec:conclusion}

We have developed two new models to reconstruct melodies, maintaining important musical structures like scale, conjunct motion and rhythmical motifs. 
The best version of these models have accuracies of around $\SI{37}{\%}$ for predicting pitches and of around $\SI{75}{\%}$ for predicting durations.
These versions use regularisation methods like dropout and $L_2$-penalty, and feeding in the previous output into the RNN for better error propagation, to enhance the generalisation performance. 

There are several ways to possibly achieve better results.
Instead of one-hot encoding the pitches and durations, we could instead use continuous encoding.
The model would then be able to learn the differences in pitch, instead of learning where to go for each certain pitch.
This would also make the model better at learning the scale of the tune and predicting consistent intervals. 
Chords could also be included as Zimmerman \cite{Zimmerman2016} did, since depending on which chord is playing, some pitches would be more probable than others.
Another thing to include are the positions in the each measure of the melody. This would essentially be a sum of the durations, and it would help with the rhythm, especially since some notes ought to fall on certain beats.
Lastly, backwards conditioning the note on the following notes through a backwards GRU layer could also aid the model prediction.

On a longer timescale, it would be interesting to extend the model to a semi-supervised variational auto-encoder and include the collections in the data set to separate genres from the melodies.
The musical notation could also be combined with sound (as with speech and text) to be able to synthesise different instruments playing.
Since the third model does not use any input but the first note, we would also like to use this for generating music, once it performs better.

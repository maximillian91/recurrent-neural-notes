\section{conclusion}
\label{sec:conclusion}

We have developed two new models to reconstruct melodies.
\change{Add comments about what the model can recognise and reconstruct (scale, conjunct motion, rhythm motifs)}
The best version of these models have accuracies of around $\SI{37}{\%}$ for predicting pitches and of around $\SI{75}{\%}$ for predicting durations.
\change{Add comments about versions.}
By using the previous output, the extended model can also generate music.
\change{Add comment about generating music.}

There are several ways to possibly achieve better results.
Instead of one-hot encoding the pitches and durations, we could instead use continuous encoding.
The model would then be able to learn the differences in pitch, instead of learning where to go for each certain pitch.
This would also make the model better learn the scale of the tune.
Chords could also be included as Zimmerman \cite{Zimmerman2016} did, since depending on which chord is playing, some pitches would be more probable than others.
Another thing to include are the positions in the each measure of the melody. This would essentially be a sum of the durations, and it would help with the rhythm, especially since some notes ought to fall on certain beats.
Lastly, knowing which notes, the melody is build towards, would also aid the model, so a backwards GRU layer could also be added to the models.

On a longer timescale, it would be interesting to extend the model to a semi-supervised variational auto-encoder and include the collections in the data set to separate genres from the melodies.
The musical notation could also be combined with sound (as with speech and text) to be able to synthesise different instruments playing.
